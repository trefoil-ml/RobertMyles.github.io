<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Stan on Lithium Theme</title>
    <link>/tags/stan/</link>
    <description>Recent content in Stan on Lithium Theme</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/tags/stan/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Stan or JAGS for Bayesian ideal-point IRT?</title>
      <link>/1/01/01/stan-or-jags-for-bayesian-ideal-point-irt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/stan-or-jags-for-bayesian-ideal-point-irt/</guid>
      <description>&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Anybody who has ever tried to run even a moderately-sized Bayesian IRT model in R (for ideal points as in the political science literature, or otherwise) will know that these models can take a &lt;em&gt;long&lt;/em&gt; time. It&amp;rsquo;s not R&amp;rsquo;s fault: these are usually big models with lots of parameters, and naturally take longer.&lt;sup id=&#34;a1&#34;&gt;&lt;a href=&#34;#fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Not to mention the fact that Bayesian computation is more computationally intense than other methods. Historically (okay, I&amp;rsquo;m talking about the last twenty years, maybe &amp;lsquo;historically&amp;rsquo; is a little strong), the sampling software &lt;a href=&#34;http://www.mrc-bsu.cam.ac.uk/software/bugs/&#34;&gt;BUGS&lt;/a&gt; (&lt;strong&gt;B&lt;/strong&gt;ayesian &lt;strong&gt;I&lt;/strong&gt;nference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling) and then &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt; were used to run Bayesian models (JAGS is still pretty common, and BUGS too, though not as much). Lately, &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; has been gaining ground, certainly as regards more complex modelling.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;While the reasons for choosing Stan are often put down to speed, when running many types of models there is not actually a large difference, with JAGS actually being faster for some models, according to John Kruschke&lt;sup id=&#34;a2&#34;&gt;&lt;a href=&#34;#fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Given the lack of a big difference between JAGS/BUGS and Stan, which sampling software should we use for IRT models? Well, first of all, a large part of the literature utilises either JAGS or BUGS, indeed, code is publicly available for many of these models, helping to spread the use of these two modelling languages.&lt;sup id=&#34;a3&#34;&gt;&lt;a href=&#34;#fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; For beginners, this is a handy way to learn, and it&amp;rsquo;s how I learned. Indeed, the language of JAGS/BUGS (I&amp;rsquo;m just going to use &amp;lsquo;JAGS&amp;rsquo; to refer to both from now on) is a bit more intuitive for many people, and given the availability of others&amp;rsquo; code, beginning with these models can then be reduced to just tinkering with small details of code that is already written.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;Stan, on the other hand, is newer and has a syntax that is in some ways quite different from JAGS. Variables need to be declared, as does their type (something not many R users are familiar with, I certainly wasn&amp;rsquo;t). The model code is imperative, not declarative&lt;sup id=&#34;a4&#34;&gt;&lt;a href=&#34;#fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, and there are specific &amp;lsquo;blocks&amp;rsquo; to the code. Stan has a different default sampler and is generally argued by its creators to be much faster. Well, in my experience, there is actually no contest. As much as I liked JAGS when I started out, Stan is simply incomparable to JAGS in terms of speed for these models&amp;ndash; Stan is much, much faster. I was analysing nominal vote data for the Brazilian Federal Senate&lt;sup id=&#34;a5&#34;&gt;&lt;a href=&#34;#fn5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; (these data have plenty of missing values, which are handled easily in JAGS but have to be deleted out in Stan) and, through the use of the &lt;a href=&#34;http://runjags.sourceforge.net/quickjags.html&#34;&gt;runjags&lt;/a&gt; package (and its &lt;code&gt;autorun&lt;/code&gt; option), I discovered that it would take around 28 hours to run my two-dimensional model to reach signs of convergence (or signs of non-convergence, as &lt;a href=&#34;pan.oxfordjournals.org/content/16/2/153.full.pdf&#34;&gt;Gill&lt;/a&gt; puts it). As I was in the middle of writing a PhD thesis with lots of these models to process, that just wasn&amp;rsquo;t an option. (Regardless, any time I let the model run like this, R crashed or became unresponsive, or the estimates were simply of bad quality.) So I started tinkering with the options in &lt;code&gt;runjags&lt;/code&gt;, trying different samplers etc. Then I noticed exactly &lt;em&gt;why&lt;/em&gt; JAGS is so slow for these models.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;In order to run a model, JAGS first compiles a Directed Acyclic Graph (DAG) of all the nodes in the model (software such as &lt;a href=&#34;http://r-nimble.org/&#34;&gt;NIMBLE&lt;/a&gt; will let you print out the graph pretty easily). But since we have a &lt;em&gt;latent&lt;/em&gt; regression with an &lt;em&gt;unobserved&lt;/em&gt; regressor in the equation&lt;sup id=&#34;a6&#34;&gt;&lt;a href=&#34;#fn6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;
 {% raw %}
  $$y_{ij} = \beta_j\bf{x_i} - \alpha_j$$
 {% endraw %}&lt;/p&gt;

&lt;p&gt;then JAGS is &lt;a href=&#34;https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/5c9e9026/&#34;&gt;unable&lt;/a&gt; to build such a DAG. Since it can&amp;rsquo;t build a DAG, it can&amp;rsquo;t surmise that there is conjugacy in the model and then exploit that through Gibbs sampling. So JAGS just uses the default Metropolis-Hastings sampler (and given that it is called &lt;strong&gt;J&lt;/strong&gt;ust &lt;strong&gt;A&lt;/strong&gt;nother &lt;strong&gt;Gibbs&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt;ampler, it kind of misses the point of using JAGS in the first place). This means that all the gains available through Gibbs sampling are simply not available for latent models of this type with JAGS, and hence the sampling process runs &lt;em&gt;very&lt;/em&gt; slowly. I&amp;rsquo;m not sure the literature was ever aware of this fact, either. Many papers and books extoll the virtues of Gibbs sampling (and spend pages and pages deriving the conditional distributions involved) and then show the reader how to do it in JAGS or BUGS (see Simon Jackman&amp;rsquo;s &lt;a href=&#34;http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470011548.html&#34;&gt;book&lt;/a&gt; for an example)&lt;sup id=&#34;a7&#34;&gt;&lt;a href=&#34;#fn7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;, but unbeknownst to these authors, their JAGS programs are not using Gibbs sampling.&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;So that leaves us with Stan. Use it! :smile:&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In a future post, I&amp;rsquo;ll show some examples of IRT ideal-point models in Stan. I have some on my &lt;a href=&#34;https://github.com/RobertMyles/Bayesian-Ideal-Point-IRT-Models&#34;&gt;Github&lt;/a&gt;, and Pablo Barberá also has some nice &lt;a href=&#34;https://github.com/pablobarbera/quant3materials/tree/master/bayesian&#34;&gt;examples&lt;/a&gt; (hat tip: I learned from him, amongst others. Thanks, Pablo!).&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update: &lt;a href=&#34;https://github.com/duarteguilherme/Quinn-Martin-Replication&#34;&gt;Guilherme Jardim Duarte&lt;/a&gt; also has some Bayesian IRT examples on his Github, in particular the rather tricky dynamic model of &lt;a href=&#34;http://mqscores.berkeley.edu/media/pa02.pdf&#34;&gt;Martin &amp;amp; Quinn&lt;/a&gt;,  have a look.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn1&#34;&gt;1&lt;/b&gt; For more on how these models can have &lt;em&gt;tons&lt;/em&gt; of parameters, see &lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/ClintonJackmanRivers2004.pdf&#34;&gt;Clinton, Jackman, and Rivers (2004)&lt;/a&gt;: &amp;lsquo;The statistical analysis of roll-call data&amp;rsquo;, &lt;em&gt;American Political Science Review&lt;/em&gt;, Vol. 98, No. 2. &lt;a href=&#34;#a1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn2&#34;&gt;2&lt;/b&gt;  &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com.br/&#34;&gt;Kruschke&lt;/a&gt; mentions this in his book&amp;hellip;not sure where, exactly. &lt;a href=&#34;#a2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn3&#34;&gt;3&lt;/b&gt; See this paper by &lt;a href=&#34;https://www.jstatsoft.org/article/view/v036c01/v36c01.pdf&#34;&gt;Curtis&lt;/a&gt; (pdf downloads automatically) or the book by &lt;a href=&#34;https://www.crcpress.com/Analyzing-Spatial-Models-of-Choice-and-Judgment-with-R/Armstrong-II-Bakker-Carroll-Hare-Poole-Rosenthal/9781466517158&#34;&gt;Armstrong et. al&lt;/a&gt;. &lt;a href=&#34;#a3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn4&#34;&gt;4&lt;/b&gt; See &lt;a href=&#34;http://stackoverflow.com/questions/129628/what-is-declarative-programming&#34;&gt;here&lt;/a&gt; for the difference.&lt;a href=&#34;#a4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn5&#34;&gt;5&lt;/b&gt; You can read about this research &lt;a href=&#34;{{ site.url }}/assets/Explaining the Determinants of Foreign Policy Voting Behaviour in the Brazilian Houses of Legislature.pdf&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#a5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn6&#34;&gt;6&lt;/b&gt; This is the canonical statistical model for Bayesian IRT. The data ({% raw %}\(y_{ij}\){% endraw %}) are the votes, in binary form (1 = &amp;lsquo;Yes&amp;rsquo;; 2 = &amp;lsquo;No&amp;rsquo;); the {% raw %}\(\bf x_i\){% endraw %} are the ideal points of the legislators; and  {% raw %} \(\beta_j\){% endraw %} and {% raw %}\(\alpha_j\){% endraw %} are the &lt;em&gt;discrimination&lt;/em&gt; (slope) and &lt;em&gt;difficulty&lt;/em&gt; (intercept) parameters, respectively. See the article cited in footnote 1.
  &lt;a href=&#34;#a6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;b id=&#34;fn7&#34;&gt;7&lt;/b&gt; I don&amp;rsquo;t mean to denigrate Jackman&amp;rsquo;s book. It&amp;rsquo;s highly detailed and thorough, and he deserves a lot of credit for spearheading the use of these Bayesian IRT models in political science. I&amp;rsquo;ve cited his work numerous times, I&amp;rsquo;m a fan.&lt;a href=&#34;#a7&#34;&gt;↩&lt;/a&gt;
&lt;link rel=&#34;image_src&#34; href=&#34;http://i.imgur.com/v7y6SVt.png?1&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stan or JAGS for Bayesian ideal-point IRT?</title>
      <link>/1/01/01/stan-or-jags-for-bayesian-ideal-point-irt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/stan-or-jags-for-bayesian-ideal-point-irt/</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Anybody who has ever tried to run even a moderately-sized Bayesian IRT model in R (for ideal points as in the political science literature, or otherwise) will know that these models can take a &lt;em&gt;long&lt;/em&gt; time. It’s not R’s fault: these are usually big models with lots of parameters, and naturally take longer.&lt;sup id=&#34;a1&#34;&gt;&lt;a href=&#34;#fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Not to mention the fact that Bayesian computation is more computationally intense than other methods. Historically (okay, I’m talking about the last twenty years, maybe ‘historically’ is a little strong), the sampling software &lt;a href=&#34;http://www.mrc-bsu.cam.ac.uk/software/bugs/&#34;&gt;BUGS&lt;/a&gt; (&lt;strong&gt;B&lt;/strong&gt;ayesian &lt;strong&gt;I&lt;/strong&gt;nference &lt;strong&gt;U&lt;/strong&gt;sing &lt;strong&gt;G&lt;/strong&gt;ibbs &lt;strong&gt;S&lt;/strong&gt;ampling) and then &lt;a href=&#34;http://mcmc-jags.sourceforge.net/&#34;&gt;JAGS&lt;/a&gt; were used to run Bayesian models (JAGS is still pretty common, and BUGS too, though not as much). Lately, &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; has been gaining ground, certainly as regards more complex modelling.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;While the reasons for choosing Stan are often put down to speed, when running many types of models there is not actually a large difference, with JAGS actually being faster for some models, according to John Kruschke&lt;sup id=&#34;a2&#34;&gt;&lt;a href=&#34;#fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;. Given the lack of a big difference between JAGS/BUGS and Stan, which sampling software should we use for IRT models? Well, first of all, a large part of the literature utilises either JAGS or BUGS, indeed, code is publicly available for many of these models, helping to spread the use of these two modelling languages.&lt;sup id=&#34;a3&#34;&gt;&lt;a href=&#34;#fn3&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; For beginners, this is a handy way to learn, and it’s how I learned. Indeed, the language of JAGS/BUGS (I’m just going to use ‘JAGS’ to refer to both from now on) is a bit more intuitive for many people, and given the availability of others’ code, beginning with these models can then be reduced to just tinkering with small details of code that is already written.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Stan, on the other hand, is newer and has a syntax that is in some ways quite different from JAGS. Variables need to be declared, as does their type (something not many R users are familiar with, I certainly wasn’t). The model code is imperative, not declarative&lt;sup id=&#34;a4&#34;&gt;&lt;a href=&#34;#fn4&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, and there are specific ‘blocks’ to the code. Stan has a different default sampler and is generally argued by its creators to be much faster. Well, in my experience, there is actually no contest. As much as I liked JAGS when I started out, Stan is simply incomparable to JAGS in terms of speed for these models– Stan is much, much faster. I was analysing nominal vote data for the Brazilian Federal Senate&lt;sup id=&#34;a5&#34;&gt;&lt;a href=&#34;#fn5&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; (these data have plenty of missing values, which are handled easily in JAGS but have to be deleted out in Stan) and, through the use of the &lt;a href=&#34;http://runjags.sourceforge.net/quickjags.html&#34;&gt;runjags&lt;/a&gt; package (and its &lt;code&gt;autorun&lt;/code&gt; option), I discovered that it would take around 28 hours to run my two-dimensional model to reach signs of convergence (or signs of non-convergence, as &lt;a href=&#34;pan.oxfordjournals.org/content/16/2/153.full.pdf&#34;&gt;Gill&lt;/a&gt; puts it). As I was in the middle of writing a PhD thesis with lots of these models to process, that just wasn’t an option. (Regardless, any time I let the model run like this, R crashed or became unresponsive, or the estimates were simply of bad quality.) So I started tinkering with the options in &lt;code&gt;runjags&lt;/code&gt;, trying different samplers etc. Then I noticed exactly &lt;em&gt;why&lt;/em&gt; JAGS is so slow for these models.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In order to run a model, JAGS first compiles a Directed Acyclic Graph (DAG) of all the nodes in the model (software such as &lt;a href=&#34;http://r-nimble.org/&#34;&gt;NIMBLE&lt;/a&gt; will let you print out the graph pretty easily). But since we have a &lt;em&gt;latent&lt;/em&gt; regression with an &lt;em&gt;unobserved&lt;/em&gt; regressor in the equation&lt;sup id=&#34;a6&#34;&gt;&lt;a href=&#34;#fn6&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[y_{ij} = \beta_j\bf{x_i} - \alpha_j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;then JAGS is &lt;a href=&#34;https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/5c9e9026/&#34;&gt;unable&lt;/a&gt; to build such a DAG. Since it can’t build a DAG, it can’t surmise that there is conjugacy in the model and then exploit that through Gibbs sampling. So JAGS just uses the default Metropolis-Hastings sampler (and given that it is called &lt;strong&gt;J&lt;/strong&gt;ust &lt;strong&gt;A&lt;/strong&gt;nother &lt;strong&gt;Gibbs&lt;/strong&gt; &lt;strong&gt;S&lt;/strong&gt;ampler, it kind of misses the point of using JAGS in the first place). This means that all the gains available through Gibbs sampling are simply not available for latent models of this type with JAGS, and hence the sampling process runs &lt;em&gt;very&lt;/em&gt; slowly. I’m not sure the literature was ever aware of this fact, either. Many papers and books extoll the virtues of Gibbs sampling (and spend pages and pages deriving the conditional distributions involved) and then show the reader how to do it in JAGS or BUGS (see Simon Jackman’s &lt;a href=&#34;http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470011548.html&#34;&gt;book&lt;/a&gt; for an example)&lt;sup id=&#34;a7&#34;&gt;&lt;a href=&#34;#fn7&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;, but unbeknownst to these authors, their JAGS programs are not using Gibbs sampling.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;So that leaves us with Stan. Use it! :smile:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;In a future post, I’ll show some examples of IRT ideal-point models in Stan. I have some on my &lt;a href=&#34;https://github.com/RobertMyles/Bayesian-Ideal-Point-IRT-Models&#34;&gt;Github&lt;/a&gt;, and Pablo Barberá also has some nice &lt;a href=&#34;https://github.com/pablobarbera/quant3materials/tree/master/bayesian&#34;&gt;examples&lt;/a&gt; (hat tip: I learned from him, amongst others. Thanks, Pablo!).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;em&gt;Update: &lt;a href=&#34;https://github.com/duarteguilherme/Quinn-Martin-Replication&#34;&gt;Guilherme Jardim Duarte&lt;/a&gt; also has some Bayesian IRT examples on his Github, in particular the rather tricky dynamic model of &lt;a href=&#34;http://mqscores.berkeley.edu/media/pa02.pdf&#34;&gt;Martin &amp;amp; Quinn&lt;/a&gt;, have a look.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn1&#34;&gt;1&lt;/b&gt; For more on how these models can have &lt;em&gt;tons&lt;/em&gt; of parameters, see &lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/ClintonJackmanRivers2004.pdf&#34;&gt;Clinton, Jackman, and Rivers (2004)&lt;/a&gt;: ‘The statistical analysis of roll-call data’, &lt;em&gt;American Political Science Review&lt;/em&gt;, Vol. 98, No. 2. &lt;a href=&#34;#a1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn2&#34;&gt;2&lt;/b&gt; &lt;a href=&#34;http://doingbayesiandataanalysis.blogspot.com.br/&#34;&gt;Kruschke&lt;/a&gt; mentions this in his book…not sure where, exactly. &lt;a href=&#34;#a2&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn3&#34;&gt;3&lt;/b&gt; See this paper by &lt;a href=&#34;https://www.jstatsoft.org/article/view/v036c01/v36c01.pdf&#34;&gt;Curtis&lt;/a&gt; (pdf downloads automatically) or the book by &lt;a href=&#34;https://www.crcpress.com/Analyzing-Spatial-Models-of-Choice-and-Judgment-with-R/Armstrong-II-Bakker-Carroll-Hare-Poole-Rosenthal/9781466517158&#34;&gt;Armstrong et. al&lt;/a&gt;. &lt;a href=&#34;#a3&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn4&#34;&gt;4&lt;/b&gt; See &lt;a href=&#34;http://stackoverflow.com/questions/129628/what-is-declarative-programming&#34;&gt;here&lt;/a&gt; for the difference.&lt;a href=&#34;#a4&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn5&#34;&gt;5&lt;/b&gt; You can read about this research &lt;a href=&#34;%7B%7B%20site.url%20%7D%7D/assets/Explaining%20the%20Determinants%20of%20Foreign%20Policy%20Voting%20Behaviour%20in%20the%20Brazilian%20Houses%20of%20Legislature.pdf&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#a5&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn6&#34;&gt;6&lt;/b&gt; This is the canonical statistical model for Bayesian IRT. The data ({% raw %}\(y_{ij}\){% endraw %}) are the votes, in binary form (1 = ‘Yes’; 2 = ‘No’); the {% raw %}\({% endraw %} are the ideal points of the legislators; and {% raw %} \(_j\){% endraw %} and {% raw %}\(_j\){% endraw %} are the &lt;em&gt;discrimination&lt;/em&gt; (slope) and &lt;em&gt;difficulty&lt;/em&gt; (intercept) parameters, respectively. See the article cited in footnote 1. &lt;a href=&#34;#a6&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn7&#34;&gt;7&lt;/b&gt; I don’t mean to denigrate Jackman’s book. It’s highly detailed and thorough, and he deserves a lot of credit for spearheading the use of these Bayesian IRT models in political science. I’ve cited his work numerous times, I’m a fan.&lt;a href=&#34;#a7&#34;&gt;↩&lt;/a&gt; &lt;link rel=&#34;image_src&#34; href=&#34;http://i.imgur.com/v7y6SVt.png?1&#34; /&gt;&lt;/p&gt;


&lt;!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Theme-Specific Voting in the European Parliament</title>
      <link>/1/01/01/theme-specific-voting-in-the-european-parliament/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/theme-specific-voting-in-the-european-parliament/</guid>
      <description>

&lt;p&gt;Since it&amp;rsquo;s &lt;a href=&#34;http://ec.europa.eu/eurostat/web/ess/european-statistics-day&#34;&gt;European Statistics Day&lt;/a&gt;, I thought I would make a quick post showing how to utilise some of the data that we have on the European Union in R. In particular, I will use European Parliament voting data from Simon Hix&amp;rsquo;s &lt;a href=&#34;http://personal.lse.ac.uk/hix/HixNouryRolandEPdata.HTM&#34;&gt;website&lt;/a&gt;. The data is freely available, so by copying and pasting the code below, you will be able to recreate the analysis I&amp;rsquo;ve done here.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re going to be using &lt;a href=&#34;http://mc-stan.org/&#34;&gt;Stan&lt;/a&gt; to make theme-specific ideal points for members of the European Parliament. You will need to install Stan and a C++ compiler to replicate the analysis.&lt;/p&gt;

&lt;p&gt;First, let&amp;rsquo;s load the R packages that we&amp;rsquo;re going to use. If you don&amp;rsquo;t have any of these, you will need to install them first, using either &lt;code&gt;install.packages(&amp;quot;name of package&amp;quot;)&lt;/code&gt; or by means of the &amp;lsquo;install&amp;rsquo; button on the Packages window of the RStudio IDE.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
library(data.table)
library(tidyverse)
library(dtplyr)
library(rstan)
library(stringi)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;After you download the data from Hix&amp;rsquo;s website, we can import it into R. I will then merge everything together.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}&lt;/p&gt;

&lt;p&gt;rm(list=ls())&lt;/p&gt;

&lt;p&gt;eu4 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2004Full.csv&amp;rdquo;, header=T))
eu5 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2005Full.csv&amp;rdquo;, header = T))
eu6 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2006Full.csv&amp;rdquo;, header = T))
eu7 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2007Full.csv&amp;rdquo;, header = T))
eu8 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2008Full.csv&amp;rdquo;, header = T))
eu9 &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/RCVS2009Full.csv&amp;rdquo;, header = T))&lt;/p&gt;

&lt;p&gt;eu &amp;lt;- eu4 %&amp;gt;%
  full_join(eu5) %&amp;gt;%
  full_join(eu6) %&amp;gt;%
  full_join(eu7) %&amp;gt;%
  full_join(eu8) %&amp;gt;%
  full_join(eu9) %&amp;gt;%
  select(-V1) %&amp;gt;%
  rename(voter = &lt;code&gt;Vote ID&lt;/code&gt;) %&amp;gt;%
  mutate(voter = stri_trans_general(voter, &amp;ldquo;Latin-ASCII&amp;rdquo;))&lt;/p&gt;

&lt;p&gt;rm(eu4, eu5, eu6, eu7, eu8, eu9)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Now we have a data frame of each of the 940 legislators in the database, and their votes on 6200 votes. Next we&amp;rsquo;ll create some id variables that we will use when we send the data to Stan.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
EU &amp;lt;- gather(eu, vote_id, vote, &lt;code&gt;1&lt;/code&gt;:&lt;code&gt;6200&lt;/code&gt;) %&amp;gt;%
  mutate(vote_id = as.numeric(vote_id),
         voter_id = as.numeric(as.factor(voter)))
head(EU)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Now we have each voter (the M.E.P., &lt;code&gt;voter&lt;/code&gt;), the id of the bill being voted on (&lt;code&gt;vote_id&lt;/code&gt;), how the individual voted (&lt;code&gt;vote&lt;/code&gt;) and the id of each voter. In these data, 1 is a &amp;lsquo;yes&amp;rsquo; vote, while 0 is &amp;lsquo;no&amp;rsquo;. The full list from Hix&amp;rsquo;s website contains the following info:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;The codes for the MEP vote decisions are as follows:
EP1, EP2 and EP5: 1=Yes, 2=No, 3=Abstain, 4=Present but did not vote, 0=Absent, 5=Not an MEP
EP3 and EP4: 1 = Yes, 2 = No, 3 = Abstain, 4 = Present but did not vote, 0 = either Absent or Not an MEP
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hix &amp;amp; co. also provide us with information on the specific policy area for each vote. We can import it, tidy it up a little and merge it to the data we have.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
theme &amp;lt;- as_tibble(fread(&amp;rdquo;~/Downloads/ep6/vc.csv&amp;rdquo;))&lt;/p&gt;

&lt;p&gt;theme &amp;lt;- theme %&amp;gt;%
  rename(vote_id = &lt;code&gt;Vote Id&lt;/code&gt;) %&amp;gt;%
  select(vote_id, Title, &lt;code&gt;Policy Area&lt;/code&gt;, Result) %&amp;gt;%
  rename(topic = &lt;code&gt;Policy Area&lt;/code&gt;) %&amp;gt;%
  mutate(topic_id = as.numeric(as.factor(topic)))&lt;/p&gt;

&lt;p&gt;rollcalls &amp;lt;- full_join(EU, theme) %&amp;gt;%
  mutate(vote = ifelse(vote==1, 1, ifelse(vote==0, 0, NA))) %&amp;gt;%
  filter(!is.na(vote))
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Next, we need to prepare the data for Stan. Our model is a basic &lt;a href=&#34;https://www.cs.princeton.edu/courses/archive/fall09/cos597A/papers/ClintonJackmanRivers2004.pdf&#34;&gt;2-parameter Item-Response theory model&lt;/a&gt; often used for &lt;a href=&#34;http://robertmyles.github.io/Bayesian-IRT-in-R-and-Stan.html&#34;&gt;creating ideal points&lt;/a&gt;. We write this in the Stan modelling language and save it as a string in R. In mathematical notation, the model is:&lt;/p&gt;

&lt;p&gt;$$y_{ijk} = \beta&lt;em&gt;j \theta&lt;/em&gt;{ik} - \alpha_j,$$&lt;/p&gt;

&lt;p&gt;where &lt;em&gt;i&lt;/em&gt; is an index of voters, &lt;em&gt;j&lt;/em&gt; an index of votes, and &lt;em&gt;k&lt;/em&gt; an index of topics. $$\theta_{ik}$$ is our main object of interest: the ideal point of MEP &lt;em&gt;i&lt;/em&gt; on topic &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;For those not familiar with Stan, the following Stan code has a &lt;code&gt;data&lt;/code&gt; block, in which we declare what our variables are and their type (these are created in the section after). Then we have a parameters block where we declare our parameters.&lt;/p&gt;

&lt;p&gt;Lastly, we have the model block where we have our model and the priors for each parameter. In an IRT model like this, we need to constrain the ideal points of at least 2 legislators. Since I am not an expert on these MEPs, I am just going to do this for the first two in the database (&lt;code&gt;theta[1]&lt;/code&gt; and &lt;code&gt;theta[2]&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
mep_model &amp;lt;- &amp;ldquo;
data {
  int&lt;lower=1&gt; J;               //MEPs
  int&lt;lower=1&gt; M;               //Proposals
  int&lt;lower=1&gt; K;               //no. of topics
  int&lt;lower=1&gt; N;               //no. of observations
  vector[K] m0;                 // prior mean for theta
  cov_matrix[K] M0;             // prior covar. for theta
  int&lt;lower=1, upper=J&gt; j[N];   //MEP for observation n
  int&lt;lower=1, upper=M&gt; m[N];   //proposal for observation n
  int&lt;lower=1, upper=K&gt; k[N];   //topic for observation n
  int&lt;lower=0, upper=1&gt; Y[N];   //vote of observation n
}
parameters {
  real alpha[M];
  real beta[M];
  vector[K] theta[J];
}
model {
  beta ~ normal(0, 10);
  alpha ~ normal(0, 10);
  for (n in 1:N)
  Y[n] ~ bernoulli_logit(theta[j[n], k[n]]*beta[m[n]] - alpha[m[n]]);
  theta ~ multi_normal(m0, M0);
  theta[1,1] ~ normal(1, .01);
  theta[2,1] ~ normal(-1, .01);
}&amp;rdquo;
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Above, we have variables declared in our Stan model. Here, I define these objects in R. All of this then goes as a list to Stan.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
library(rstan)&lt;/p&gt;

&lt;p&gt;N &amp;lt;- nrow(rollcalls)
M &amp;lt;- max(rollcalls$vote_id)
K &amp;lt;- max(rollcalls$topic_id)
J &amp;lt;- max(rollcalls$voter_id)
Y &amp;lt;- rollcalls$vote
m &amp;lt;- rollcalls$vote_id
k &amp;lt;- rollcalls$topic_id
j &amp;lt;- rollcalls$voter_id&lt;/p&gt;

&lt;h1 id=&#34;mean-and-covariances-for-theta&#34;&gt;Mean and Covariances for theta&lt;/h1&gt;

&lt;p&gt;m0 &amp;lt;- rep(0, times=K)
M0 &amp;lt;- matrix(0, K, K)
diag(M0) &amp;lt;- 1&lt;/p&gt;

&lt;p&gt;stan_data &amp;lt;- list(J=J, N=N, M=M, j=j,
                  Y=Y, m=m, K=K, k=k,
                  m0=m0, M0=M0)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;Next, we run our model with Stan. Here I use Stan&amp;rsquo;s new [ADVI]() feature, but the Stan folks don&amp;rsquo;t recommend this for inference. However, for a blog post it&amp;rsquo;s okay :smile:.&lt;/p&gt;

&lt;p&gt;ADVI is much faster than the already comparatively fast NUTS sampling that Stan does. Here we have a lot of data, though, so this next part will take &lt;strong&gt;a few hours&lt;/strong&gt; to run. If you don&amp;rsquo;t fancy waiting so long, subset the data (maybe choose just one year) and run the Stan code on the smaller dataset.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
Stan_Model &amp;lt;- stan_model(model_name = &amp;ldquo;meps&amp;rdquo;, model_code = mep_model)&lt;/p&gt;

&lt;p&gt;Res1 &amp;lt;- vb(Stan_Model, data = stan_data, seed = 1234,
          init = &amp;ldquo;random&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;So one thing that we could do with the estimates from this model is plot the ideal points of an MEP as they vary over the themes that he/she voted on.&lt;/p&gt;

&lt;p&gt;What we will do is extract the elements of the summary that we want and then create the summary that we need to start making figures.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
summary &amp;lt;- list(summary(Res1, pars=&amp;ldquo;theta&amp;rdquo;))
summary &amp;lt;- summary[[1]][1]
summary &amp;lt;- as_data_frame(summary[[1]]) %&amp;gt;%
  mutate(names = theta_names,
         voters = rep(unique(rollcalls$voter), each=21),
         index = as.character(str_extract_all(names, &amp;ldquo;\.[0-9]*$&amp;ldquo;)),
         index = gsub(&amp;rdquo;\.&amp;ldquo;, &amp;ldquo;&amp;rdquo;, index),
         index = as.integer(index))&lt;/p&gt;

&lt;p&gt;topics &amp;lt;- unique(rollcalls$topic)
index &amp;lt;- unique(summary$index)
topic_index &amp;lt;- tibble(topic = topics, index = index)&lt;/p&gt;

&lt;p&gt;mep_summary &amp;lt;- full_join(summary, topic_index) %&amp;gt;%
  select(-c(names, index))&lt;/p&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;The following graphs are of Adamos Adamou and Filip Adwent, for the simple reason that they are the first names in the database. First, we create a plot for Adamou and then for Adwent, then we combine them. In the following code, I customize the font, but none of that is necessary.&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
adamou &amp;lt;- mep_summary %&amp;gt;% filter(voters==&amp;ldquo;ADAMOU, Adamos&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;ggplot(adamou, aes(x = mean, y = topic)) +
  geom_segment(aes(yend = topic), color = &amp;ldquo;#104E8B&amp;rdquo;,
               xend = 0, alpha = 0.3) +
  geom_point(size = 4, color = &amp;ldquo;#104E8B&amp;rdquo;) + theme_bw() +
  theme(legend.position = &amp;ldquo;none&amp;rdquo;, axis.title.y = element_blank(),
        axis.title.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, face=&amp;lsquo;bold&amp;rsquo;),
        axis.text.y = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12),
        axis.text.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12)) +
  xlab(&amp;ldquo;Ideal Points, Adamos Adamou&amp;rdquo;) +
  geom_vline(xintercept = 0, linetype = &amp;ldquo;dashed&amp;rdquo;)
{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/C4zpATc.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}
adwent &amp;lt;- mep_summary %&amp;gt;% filter(voters==&amp;ldquo;ADWENT, Filip&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;ggplot(adwent, aes(x = mean, y = topic)) +
  geom_segment(aes(yend = topic), color = &amp;ldquo;#8B1A1A&amp;rdquo;,
               xend = 0, alpha = 0.3) +
  geom_point(size = 4, color = &amp;ldquo;#8B1A1A&amp;rdquo;) + theme_bw() +
  theme(legend.position = &amp;ldquo;none&amp;rdquo;, axis.title.y = element_blank(),
        axis.title.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, face=&amp;lsquo;bold&amp;rsquo;),
        axis.text.y = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12),
        axis.text.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12)) +
  xlab(&amp;ldquo;Ideal Points, Filip Adwent&amp;rdquo;) +
  geom_vline(xintercept = 0, linetype = &amp;ldquo;dashed&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/gKFgL5L.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;We can put these two together and see how they compare:&lt;/p&gt;

&lt;p&gt;{% highlight R linenos=table %}&lt;/p&gt;

&lt;p&gt;ggplot(adamou, aes(x = mean, y = topic)) +
  geom_segment(aes(yend = topic), color = &amp;ldquo;#104E8B&amp;rdquo;,
               xend = 0, alpha = 0.3) +
  geom_segment(data = adwent, aes(yend = topic),
               xend = 0, colour = &amp;ldquo;#8B1A1A&amp;rdquo;,
               alpha = 0.3) +
  geom_point(size = 4, color = &amp;ldquo;#104E8B&amp;rdquo;) + theme_bw() +
  theme(legend.position = &amp;ldquo;none&amp;rdquo;, axis.title.y = element_blank(),
        axis.title.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, face=&amp;lsquo;bold&amp;rsquo;),
        axis.text.y = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12),
        axis.text.x = element_text(family = &amp;ldquo;Georgia&amp;rdquo;, size = 12)) +
  xlab(&amp;ldquo;Ideal Points, Adamos Adamou &amp;amp; Filip Adwent&amp;rdquo;) +
  geom_point(data = adwent, aes(x = mean, y =topic),
             size = 4, color = &amp;ldquo;#8B1A1A&amp;rdquo;) +
  geom_vline(xintercept = 0, linetype = &amp;ldquo;dashed&amp;rdquo;)&lt;/p&gt;

&lt;p&gt;{% endhighlight %}&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/kBq6WGv.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;And of course you can customise these ggplot figures any way you like.&lt;/p&gt;

&lt;p&gt;Happy European Statistics Day! :dancers:&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
