<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Webscraping on if(topic != junk) {write} else {:wine_glass:}</title>
    <link>/tags/webscraping/</link>
    <description>Recent content in Webscraping on if(topic != junk) {write} else {:wine_glass:}</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Aug 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/webscraping/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Easier web scraping in R</title>
      <link>/easier-web-scraping-in-r.html</link>
      <pubDate>Fri, 05 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>/easier-web-scraping-in-r.html</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In an earlier &lt;a href=&#34;http://robertmyles.github.io//Web-Navigation-and-Scraping-with-R.html&#34;&gt;post&lt;/a&gt;, I described some ways in which you can interact with a web browser using R and &lt;code&gt;RSelenium&lt;/code&gt;. This is ideal when you need to access data through drop-down menus and search bars. However, working with &lt;code&gt;RSelenium&lt;/code&gt; can be tricky. There are, of course, easier ways to get information from the internet using R.&lt;/p&gt;
&lt;p&gt;Perhaps the most straightforward way is to use &lt;code&gt;rvest&lt;/code&gt;, in tandem with other packages of the &lt;a href=&#34;https://barryrowlingson.github.io/hadleyverse/#1&#34;&gt;Hadleyverse&lt;/a&gt;&lt;sup id=&#34;a1&#34;&gt;&lt;a href=&#34;#fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, such as &lt;code&gt;dplyr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt; for data preparation and cleaning after the webscrape. I’m going to use a simple example that I came across recently in my work, getting the name of each mayor in Brazil.&lt;/p&gt;
&lt;p&gt;Finding out who was &lt;em&gt;elected&lt;/em&gt; to the mayor’s office in each municipality in Brazil is easy: that data exists and is available on the &lt;a href=&#34;http://www.tse.jus.br/&#34;&gt;website&lt;/a&gt; of the &lt;em&gt;Tribunal Superior Eleitoral&lt;/em&gt;. However, just because someone was elected to office (in this case in 2014) does not mean that they are still in office now, two years later. After searching around the web for a bit, I realised that this data is not available as a dataset.&lt;/p&gt;
&lt;p&gt;After wandering to the website of the &lt;a href=&#34;http://www.ibge.gov.br/home/&#34;&gt;IBGE&lt;/a&gt;, a Brazilian statistics agency, I found a way to get the name of the mayor currently in charge of each municipality. Each municipality has its own webpage on the IGBE’s dedicated &lt;a href=&#34;http://www.cidades.ibge.gov.br/xtras/home.php&#34;&gt;Cidades@&lt;/a&gt; site.&lt;/p&gt;
&lt;p&gt;For example, you will see the a webpage for the municipality of Acrelândia, shown in the image below. As you can see, the name of the mayor (“Prefeito”) is on the right-hand side of the page. Since we now know we can get this for each municipality, we have three tasks to do in order to get this info into R:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;find out what part of the url changes as we move from city to city on the website;&lt;/li&gt;
&lt;li&gt;send the corresponding information to the server using R;&lt;/li&gt;
&lt;li&gt;scrape the page and tidy up the resulting data in R.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/MGqKffr.png&#34; style=&#34;width:750px;height:500px;&#34;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;The url for &lt;a href=&#34;http://www.cidades.ibge.gov.br/xtras/perfil.php?lang=&amp;amp;codmun=120001&amp;amp;search=acre%7Cacrelandia&#34;&gt;Acrelândia&lt;/a&gt; is unique at: “codmun=120001” and “search=acre|acrelandia”.&lt;br /&gt;
The number in “codmun” is available as the IBGE municipal code (although missing the final digit, strangely…but that’s not a problem, we just take it off the end for each one) and the rest is just the state and the municipality, all information that is easy to get from various sources. For this example, I’ve uploaded this basic dataset to Github so we can use it here.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(dplyr)
library(tidyr)
library(readr)
library(stringr)
library(stringi)
library(rvest)

## read in data and create variables for webscraping:
Mayors &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/RobertMyles/RobertMyles.github.io/master/_data/IBGE_codes.csv&amp;quot;) %&amp;gt;% 
  select(-c(UF, Cod.Mun)) %&amp;gt;% 
  dplyr::rename(Code_IBGE = Cod.IBGE) %&amp;gt;% 
  mutate(MUNIC2 = tolower(.$MUNIC)) %&amp;gt;% 
  mutate(MUNIC2 = gsub(&amp;quot; &amp;quot;, &amp;quot;-&amp;quot;, .$MUNIC2)) %&amp;gt;% 
  mutate(Name_UF2 = tolower(.$Name_UF)) %&amp;gt;% 
  mutate(Code2 = unlist(str_extract_all(.$Code_IBGE, &amp;quot;[0-9]{6}&amp;quot;))) %&amp;gt;% 
  unite(col = Link, Name_UF2, MUNIC2, sep = &amp;quot;|&amp;quot;, remove = F) %&amp;gt;% 
  arrange(ACR_UF) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;  In the code snippet above, we’ve taken out unnecessary columns, renamed one, changed the names of the municipalities to lower case (for the url), taken six numbers of the IBGE code for use in the webscrape and joined the state and municipality names together, with &lt;code&gt;|&lt;/code&gt; seperating them, as in the url for each municipality webpage. We also need to create some empty data frames to fill, and remove the municipality of Brasília, which does not have a &lt;em&gt;Prefeito&lt;/em&gt;, just a &lt;a href=&#34;http://www.cidades.ibge.gov.br/xtras/perfil.php?lang=&amp;amp;codmun=530010&amp;amp;search=distrito-federal%7Cbrasilia&#34;&gt;governor&lt;/a&gt;, which is all done below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;url &amp;lt;- &amp;quot;http://www.cidades.ibge.gov.br/xtras/perfil.php?lang=&amp;amp;codmun=&amp;quot;

link &amp;lt;- Mayors$Link
grep(&amp;quot;distrito federal|brasilia&amp;quot;, link)
link &amp;lt;- link[-804]
link2 &amp;lt;- Mayors$Code2
link2 &amp;lt;- link2[-804]

Prefeitos &amp;lt;- data.frame()
Cidades &amp;lt;- data.frame()
Pref &amp;lt;- data.frame()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next comes our webscrape, which is incredibly easy with &lt;code&gt;rvest&lt;/code&gt; (&lt;code&gt;xml2&lt;/code&gt; is likewise easy). The only hard part of this entire scrape is getting the words “Prefeito” along with the name of the mayor out of the document. This relies on regex, which can be tricky. But trial and error should lead you to the right answer for whatever you need. Or search &lt;a href=&#34;http://www.rexegg.com/regex-quickstart.html&#34;&gt;Google&lt;/a&gt;, of course.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:length(link)){
  URL &amp;lt;- paste(url, link2[i], &amp;quot;&amp;amp;search=&amp;quot;, link[i], sep = &amp;quot;&amp;quot;)
  pref &amp;lt;- read_html(URL)
  pref1 &amp;lt;- html_nodes(pref, xpath = &amp;#39;//*[@id=&amp;quot;mod_perfil_infosbasicas&amp;quot;]&amp;#39;)
  str &amp;lt;- html_text(pref1)
  str1 &amp;lt;- unlist(str_extract_all(str, &amp;quot;Prefeito[\\w A-Z]*&amp;quot;))
  print(str1)
  Prefeitos &amp;lt;- rbind(Prefeitos, str1, stringsAsFactors = F)
  City &amp;lt;- link[i]
  Cidades &amp;lt;- rbind(Cidades, City, stringsAsFactors = F)
  Pref &amp;lt;- cbind(Prefeitos, Cidades)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With a little tidying, we have a nice little dataset of each current mayors for each municipality in Brazil.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;colnames(Pref) &amp;lt;- c(&amp;quot;Prefeito&amp;quot;, &amp;quot;Municipio&amp;quot;) 
Pref$Prefeito &amp;lt;- gsub(&amp;quot;Prefeito&amp;quot;, &amp;quot;&amp;quot;, Pref$Prefeito)
Pref$Prefeito &amp;lt;- stri_trans_general(Pref$Prefeito, &amp;quot;Latin-ASCII&amp;quot;)
Pref1 &amp;lt;- Pref 
Pref1$Municipio &amp;lt;- Pref1$Municipio %&amp;gt;% 
  str_split_fixed(&amp;quot;\\|&amp;quot;, n = 2) %&amp;gt;% 
  toupper()
Pref$Name_UF &amp;lt;- Pref1$Municipio[,1]
Pref$MUNIC &amp;lt;- Pref1$Municipio[,2]
Pref &amp;lt;- select(Pref, -Municipio)
Mayors$MUNIC &amp;lt;- gsub(&amp;quot;[-]&amp;quot;, &amp;quot; &amp;quot;, Mayors$MUNIC)
Pref$MUNIC &amp;lt;- gsub(&amp;quot;[-]&amp;quot;, &amp;quot; &amp;quot;, Pref$MUNIC)
rm(Pref1)

Prefeitos &amp;lt;- full_join(Mayors, Pref)

Prefeitos &amp;lt;- select(Prefeitos, -c(Link, MUNIC2, Name_UF2, Code2))
Prefeitos &amp;lt;- Prefeitos[,c(1:5, 7, 6)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://i.imgur.com/TRMOSkV.png&#34; style=&#34;width:650px;height:400px;&#34;&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn1&#34;&gt;1&lt;/b&gt; Supposedly, Hadley Wickham doesn’t actually like this term, but I’ll use it anyway, I’m sure he wouldn’t mind :smiley:. &lt;a href=&#34;#a1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;link rel=&#34;image_src&#34; href=&#34;http://i.imgur.com/VuCDpaX.png?1&#34; /&gt;&lt;/p&gt;


&lt;!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
    <item>
      <title>Web Navigation in R with RSelenium</title>
      <link>/web-navigation-and-scraping-with-r.html</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/web-navigation-and-scraping-with-r.html</guid>
      <description>&lt;!-- BLOGDOWN-BODY-BEFORE

/BLOGDOWN-BODY-BEFORE --&gt;

&lt;p&gt; &lt;/p&gt;
&lt;p&gt;It goes almost without saying that the internet itself is the richest database available to us. From a 2014 &lt;a href=&#34;http://aci.info/2014/07/12/the-data-explosion-in-2014-minute-by-minute-infographic/&#34;&gt;blog post&lt;/a&gt;, it was claimed that &lt;em&gt;every minute&lt;/em&gt; :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Facebook users share nearly 2.5 million pieces of content.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Twitter users tweet nearly 300,000 times.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Instagram users post nearly 220,000 new photos.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;YouTube users upload 72 hours of new video content.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Apple users download nearly 50,000 apps.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Email users send over 200 million messages.&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Amazon generates over $80,000 in online sales.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Regardless of the accuracy of these claims, it is obvious to everyone that there is tons of information on the web. For researchers, then, the question is: how can you access all this information? You can of course go to specific, dedicated databases and download what you’re looking for, for example from the World Bank &lt;a href=&#34;http://databank.worldbank.org/data/home.aspx&#34;&gt;databank&lt;/a&gt;. However, there are drawbacks to this approach. It can become tiresome when you need to collect lots of data on different items (the World Bank databank is well organised, but not all databases are like that…to put it politely). Some only let you download small, specific sections of a bigger database, meaning you have to return time and time again to the starting page to enter new information in order to retrieve the data you want. (Another thing is that we’re not quite utilising the web &lt;em&gt;itself&lt;/em&gt; as the database either.)&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;To deal with the first problem, you can automate the search process by driving a web browser with R.&lt;sup id=&#34;a1&#34;&gt;&lt;a href=&#34;#fn1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; This is different from ‘web-scraping’. Web-scraping takes the webpage as a html document and allows you to read information from it. It’s quite a straightforward process, with plenty of R packages around to help you do it. &lt;a href=&#34;https://github.com/hadley/rvest&#34;&gt;rvest&lt;/a&gt; in particular is quite easy, although I’ve found the &lt;a href=&#34;https://cran.r-project.org/web/packages/XML/XML.pdf&#34;&gt;XML&lt;/a&gt; package to be more powerful. (Web-scraping deals with the second issue above, in that it does treat the web itself as a database.)&lt;/p&gt;
&lt;p&gt;To drive a web browser in R, there are two packages (that I’m aware of) that can be used. One is &lt;a href=&#34;https://github.com/ropensci/RSelenium&#34;&gt;RSelenium&lt;/a&gt; by John Harrison, and &lt;a href=&#34;https://github.com/crubba/Rwebdriver&#34;&gt;Rwebdriver&lt;/a&gt; by Christian Rubba. I prefer &lt;code&gt;RSelenium&lt;/code&gt; and so I’ll use this package in the examples below.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;If you don’t have it already installed, you’ll need to download this package and load it into R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;install.packages(&amp;quot;RSelenium&amp;quot;)
library(&amp;quot;RSelenium&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will also need to download the Selenium standalone server. You can get it from &lt;a href=&#34;http://www.seleniumhq.org/download/&#34;&gt;here&lt;/a&gt;. Opening this file automatically from &lt;code&gt;RSelenium&lt;/code&gt; can be problematic&lt;sup id=&#34;a2&#34;&gt;&lt;a href=&#34;#fn2&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;, and so I’ve found the most straightforward way is to manually click on it and open it that way before you start.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;To get started with &lt;code&gt;RSelenium&lt;/code&gt;, you’ll need to give your browser somewhere to go. For this example, I’m going to go to the funding management section of Brazilian National Health Service, the &lt;em&gt;Fundo Nacional de Saúde&lt;/em&gt;. From here, I’m going to get data for every municipality in every state over a period of some years. To do this manually would be a serious headache and would most likely lead to me making errors by forgetting where I am, which state is next, what municipality I just downloaded, and so on. Actually, you can be guaranteed I’d make those mistakes.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;URL &amp;lt;- &amp;quot;http://www.fns.saude.gov.br/indexExterno.jsf&amp;quot;
#checkForServer(dir=&amp;quot;[DIRECTORY WHERE THE SELENIUM SERVER IS]&amp;quot;, update=FALSE)
#checkForServer(dir=&amp;quot;[DIRECTORY WHERE THE SELENIUM SERVER IS]&amp;quot;, update=TRUE) # if you want to update
#startServer(dir=&amp;quot;[DIRECTORY WHERE THE SELENIUM SERVER IS]&amp;quot;) #none of these three are necessary if you click on the server first and manually open it.  

fprof &amp;lt;- makeFirefoxProfile(list(browser.download.dir = &amp;quot;[DOWNLOAD DIRECTORY]&amp;quot;,  
browser.download.folderList = 2L,   
browser.download.manager.showWhenStarting=FALSE,  
browser.helperApps.neverAsk.saveToDisk = &amp;quot;application/octet-stream&amp;quot;))   

remDr &amp;lt;- remoteDriver(extraCapabilities=fprof)
remDr$open()  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;So now your browser should be open. Here I’ve used a profile for Firefox because I will download files and I don’t want to deal with the download window that pops up in Firefox (you need to enter your download folder where it says ‘&lt;code&gt;[DOWNLOAD DIRECTORY]&lt;/code&gt;’, by the way. And you can also run &lt;code&gt;RSelenium&lt;/code&gt; on Chrome and &lt;a href=&#34;http://rpubs.com/johndharrison/13885&#34;&gt;other browsers&lt;/a&gt;, and even use a &lt;a href=&#34;https://rpubs.com/johndharrison/RSelenium-headless&#34;&gt;headless browser&lt;/a&gt; which speeds things up.) If you didn’t need to deal with download boxes and pop-ups and the like, you only need &lt;code&gt;remDr &amp;lt;- remoteDriver$new()&lt;/code&gt;, which will automatically open up a Firefox browser window. These particular files were recognised by Firefox as being binary files, and so I have disabled the download box for files of the type “application/octet-stream”. Other file types need a different setting.&lt;/p&gt;
&lt;p&gt;This website has a drop-down box on the left hand side that we’re going to use. What we will input into this is, in turn, a list of years, states, and municipalities. After that we will click on “Consultar” (for those of you who don’t speak Portuguese, I’m quite sure you can figure out what that means). Clicking this will bring us to a new page, from which we can download the data we’re looking for in a .csv file.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;So let’s create our inputs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;InputYear &amp;lt;- list(&amp;quot;2016&amp;quot;, &amp;quot;2015&amp;quot;, &amp;quot;2014&amp;quot;, &amp;quot;2013&amp;quot;, &amp;quot;2012&amp;quot;, &amp;quot;2011&amp;quot;, &amp;quot;2010&amp;quot;, &amp;quot;2009&amp;quot;)  

Input &amp;lt;- list(&amp;quot;ACRE&amp;quot;, &amp;quot;ALAGOAS&amp;quot;, &amp;quot;AMAPA&amp;quot;, &amp;quot;AMAZONAS&amp;quot;, &amp;quot;BAHIA&amp;quot;, &amp;quot;CEARA&amp;quot;, &amp;quot;DISTRITO FEDERAL&amp;quot;, &amp;quot;ESPIRITO SANTO&amp;quot;, &amp;quot;GOIAS&amp;quot;, &amp;quot;MARANHAO&amp;quot;, &amp;quot;MATO GROSSO&amp;quot;, &amp;quot;MATO GROSSO DO SUL&amp;quot;, &amp;quot;MINAS GERAIS&amp;quot;, &amp;quot;PARA&amp;quot;, &amp;quot;PARAIBA&amp;quot;, &amp;quot;PARANA&amp;quot;, &amp;quot;PERNAMBUCO&amp;quot;, &amp;quot;PIAUI&amp;quot;, &amp;quot;RIO DO JANEIRO&amp;quot;, &amp;quot;RIO GRANDE DO NORTE&amp;quot;, &amp;quot;RIO GRANDE DO SUL&amp;quot;, &amp;quot;RONDONIA&amp;quot;, &amp;quot;RORAIMA&amp;quot;, &amp;quot;SANTA CATARINA&amp;quot;, &amp;quot;SAO PAULO&amp;quot;, &amp;quot;SERGIPE&amp;quot;, &amp;quot;TOCANTINS&amp;quot;)  

Input_Mun &amp;lt;- &amp;quot;TODOS DA UF&amp;quot; #this will select all municipalities  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;In order to get all this done, I will use a for loop in R which will first loop over the years, and then states, thereby selecting all states in a given year. In the following code, you will see &lt;code&gt;RSelenium&lt;/code&gt; commands that are quite different to regular commands in R. First of all, &lt;code&gt;RSelenium&lt;/code&gt; operates by way of two environments: one is remoteDriver environment, the other a webElement environment. These have specific options available to them (see the help section on each for a list and explanations). Some of the most useful are &lt;code&gt;findElement()&lt;/code&gt; (an option of remoteDriver), &lt;code&gt;sendKeystoElement()&lt;/code&gt; and &lt;code&gt;clickElement()&lt;/code&gt; (both options of webElement, as &lt;code&gt;remDr$findElement&lt;/code&gt; returns an object of webElement class). We will use these to navigate around the page and click on specific elements.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Speaking of elements on a page, this is actually the most crucial part of the process to get right (and can be the most frustrating). Some have recommended &lt;a href=&#34;http://selectorgadget.com/&#34;&gt;selectorgadget&lt;/a&gt;, but finding elements can be done in Firefox or Chrome without selectorgadget – you just right-click the element in question and select “Inspect” or “Inspect Element”. This will bring up a chaotic-looking panel, full of html, css and javascript code. Luckily, there are easy options in Firefox and Chrome for finding what we need. After you right-click the element that you want (the one you would have clicked if you were navigating the page manually), click “Inspect” and then this element of the html code will be highlighted. Right-click on this again and you will see the option to copy. In Chrome, you will have the option to copy the xpath or css selector (“selector”); in Firefox you can copy the css selector (“unique selector”). I have used other options below to give more examples, such as ‘id’. This can be copied directly from the html code, and ‘class’ and ‘name’ can be used in a similar fashion. In general, css selectors are the easiest to work with.&lt;/p&gt;
&lt;p&gt;A quick note on some other aspects of the code. &lt;code&gt;Sys.sleep&lt;/code&gt; is used in order to be nice– you don’t want to bombard the website with all of your requests in rapid-fire fashion; after all, they may block you. So this spaces out our commands. This is also useful for when you may have to wait for an element to load on the page before you can click on it. I have used &lt;code&gt;paste()&lt;/code&gt; in order to include the loop counters in the css selector– just a little trick to make things easier. Some elements have &lt;code&gt;\\&lt;/code&gt; in the code: this is because the original had a single backslash, which is an escape character in R, and so the string is unreadable. Hence the added backslash. You will also see the use of &lt;code&gt;try()&lt;/code&gt; – in this case, there is a state that does not load like the others (the Federal District) and so this automated process will not work here. &lt;code&gt;try()&lt;/code&gt; allows R to try anyway, and if it fails, the loop just continues to the next iteration.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;for(i in 1:length(InputYear)){
  for(j in 1:length(Input)){
    remDr$navigate(URL)
    #Year:
    webElem &amp;lt;- remDr$findElement(using = &amp;quot;id&amp;quot;, value = &amp;quot;formIndex:j_idt48&amp;quot;)
    webElem$clickElement() #click on the drop-down year box
    Sys.sleep(2)
    webElem &amp;lt;- remDr$findElement(using = &amp;quot;id&amp;quot;, value=&amp;quot;formIndex:j_idt48_input&amp;quot;)
    Sys.sleep(2)
    webElem$sendKeysToElement(InputYear[i]) #send the year to the box
    webElem &amp;lt;- remDr$findElement(using = &amp;quot;css&amp;quot;, value=&amp;quot;li.ui-state-active&amp;quot;)
    webElem$clickElement() #click on the active element (the year we sent)
    Sys.sleep(2)
    #State:
    webElem &amp;lt;- remDr$findElement(using = &amp;quot;id&amp;quot;, value = &amp;quot;formIndex:sgUf&amp;quot;)
    webElem$clickElement()
    Sys.sleep(2)
    webElem$sendKeysToElement(Input[j]) #enter the state into the drop-down box
    CSS &amp;lt;- paste(&amp;quot;#formIndex\\3a sgUf_panel &amp;gt; div &amp;gt; ul &amp;gt; li:nth-child(&amp;quot;, j+2, &amp;quot;)&amp;quot;, sep=&amp;quot;&amp;quot;)
    webElem &amp;lt;- remDr$findElement(using = &amp;quot;css&amp;quot;, value = CSS)
    Sys.sleep(1)
    webElem$clickElement()
    Sys.sleep(3)
    #Municipality:
    webElem &amp;lt;- remDr$findElement(using = &amp;#39;id&amp;#39;, value = &amp;#39;formIndex:cbMunicipio&amp;#39;)
    webElem$clickElement()
    Sys.sleep(2)
    webElem &amp;lt;- remDr$findElement(using = &amp;#39;css&amp;#39;, value=&amp;#39;#formIndex\\3a cbMunicipio_panel &amp;gt; div &amp;gt; ul &amp;gt; li:nth-child(2)&amp;#39;)
    webElem$sendKeysToElement(list(Input_Mun))
    webElem$clickElement()
    Sys.sleep(4)
    #&amp;quot;Consultar&amp;quot;:
    webElem &amp;lt;- remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;formIndex:j_idt60&amp;quot;]&amp;#39;)
    Sys.sleep(2)
    webElem$clickElement() 
    Sys.sleep(6)
    #Download the .csv:
    webElem &amp;lt;- try(remDr$findElement(using = &amp;#39;xpath&amp;#39;, value = &amp;#39;//*[@id=&amp;quot;formIndex&amp;quot;]/div[4]/input&amp;#39;), silent=T)
    try(webElem$clickElement(), silent=T)
    Sys.sleep(3)
}}  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;So after all this, we’ll have a bunch of .csv files in out download folder, that you can import into R and mess around with. To load them all in together, you could use the following code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(&amp;quot;readr&amp;quot;)
setwd(&amp;quot;[THE DOWNLOAD FOLDER YOU USED]&amp;quot;)
fileNames &amp;lt;- list.files(path = getwd(), pattern = &amp;quot;*.csv&amp;quot;)
data &amp;lt;- rbindlist(lapply(fileNames, read_csv2,  
col_names=c(&amp;quot;Ano&amp;quot;, &amp;quot;UF_MUNICIPIO&amp;quot;, &amp;quot;IBGE&amp;quot;, &amp;quot;ENTIDADE&amp;quot;, &amp;quot;CPF_CNPJ&amp;quot;,  
&amp;quot;Bloco&amp;quot;, &amp;quot;Componente&amp;quot;, &amp;quot;Acao_Servico_Estrategia&amp;quot;, &amp;quot;Competencia_Parcela&amp;quot;,  
&amp;quot;No_OB&amp;quot;, &amp;quot;Data_OB&amp;quot;, &amp;quot;Banco_OB&amp;quot;, &amp;quot;Agencia_OB&amp;quot;, &amp;quot;Conta_OB&amp;quot;, &amp;quot;Valor_Total&amp;quot;,  
&amp;quot;Desconto&amp;quot;, &amp;quot;Valor_Liquido&amp;quot;, &amp;quot;Observacao&amp;quot;, &amp;quot;Processo&amp;quot;, &amp;quot;Tipo Repasse&amp;quot;,  
&amp;quot;No_Proposta&amp;quot;), skip = 1, locale=locale(decimal_mark=&amp;quot;,&amp;quot;, grouping_mark=&amp;quot;.&amp;quot;)))  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;And there you go, all the data you wanted scraped automatically from the web. In this example, we were downloading a file, but you could be navigating around in order to arrive at a certain page and then to scrape the contents of that page. You can do that in a number of ways, by combining &lt;code&gt;RSelenium&lt;/code&gt; and other packages, such as &lt;code&gt;XML&lt;/code&gt; and &lt;code&gt;rvest&lt;/code&gt;. For a solution using only &lt;code&gt;RSelenium&lt;/code&gt;, we can first create an empty dataframe and then fill it with the &lt;code&gt;getElementText()&lt;/code&gt; option of the webElement class. So, for example, I was getting vote proposal content from the Brazilian Senate. I used &lt;code&gt;RSelenium&lt;/code&gt; to navigate to the pages that I wanted, as is shown above, and then I stored the Content and the Index of the vote (which were stored on the page as html text elements) as entries in the Index dataframe, using &lt;code&gt;webElem$getElementText()&lt;/code&gt;. Afterwards, I used various combinations of &lt;code&gt;stringr&lt;/code&gt; package functions and &lt;code&gt;gsub&lt;/code&gt; to clean up the text.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;Index &amp;lt;- data.frame(Content=NA, Index=NA)  
Index[i,1] &amp;lt;- webElem$getElementText()  
   ...    
Index[i,2] &amp;lt;- webElem$getElementText()  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;You can also get the html and parse it using &lt;code&gt;XML&lt;/code&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;elemtxt &amp;lt;- webElem$getElementAttribute(&amp;quot;outerHTML&amp;quot;)  
elemxml &amp;lt;- htmlTreeParse(elemtxt, asText=TRUE, encoding=&amp;quot;UTF-8&amp;quot;, useInternalNodes=TRUE)  
Text &amp;lt;- html_text(elemxml, trim=TRUE)  &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;And then you have the text from the webpage stored as data in R. Magic! :metal:&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn1&#34;&gt;1&lt;/b&gt; It is often argued that R is not the best for this application, with Python often offered as a better alternative. In my experience, I’ve found R to be pretty good for this sort of thing, with delays being caused more by the browser/net speed than R. The scripts can be ugly, but using Selenium in Python looks pretty similar anyway. &lt;a href=&#34;http://stackoverflow.com/questions/17540971/how-to-use-selenium-with-python&#34;&gt;This question&lt;/a&gt; on Stack Overflow gives some instructions. &lt;a href=&#34;#a1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;b id=&#34;fn2&#34;&gt;2&lt;/b&gt; See &lt;a href=&#34;https://github.com/ropensci/RSelenium/issues/54&#34;&gt;this&lt;/a&gt; discussion. &lt;a href=&#34;#a2&#34;&gt;↩&lt;/a&gt; &lt;link rel=&#34;image_src&#34; href=&#34;http://i.imgur.com/v7y6SVt.png?1&#34; /&gt;&lt;/p&gt;


&lt;!-- BLOGDOWN-HEAD




/BLOGDOWN-HEAD --&gt;
</description>
    </item>
    
  </channel>
</rss>
